# âš¡ Quick Start - Test Local avec Ollama

**DÃ©marrage rapide en 5 minutes**

---

## ðŸš€ Installation Express

### 1. Installer Ollama (2 min)

**Windows/Mac** : TÃ©lÃ©charger depuis https://ollama.com/download

**Linux** :
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### 2. TÃ©lÃ©charger un ModÃ¨le (3-5 min)

```bash
# ModÃ¨le lÃ©ger (recommandÃ©)
ollama pull llama3.2:1b
```

### 3. Configurer le Backend (1 min)

```bash
cd backend
cp .env.example .env

# Ã‰diter .env et ajouter :
# OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_MODEL=llama3.2:1b
```

### 4. DÃ©marrer (1 min)

```bash
python scripts/start_server.py
```

**C'est tout ! ðŸŽ‰**

---

## âœ… Tester

```bash
# Health
curl http://localhost:8000/api/health

# Chat avec Ollama (gratuit, local)
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"Hello","language":"en"}'

# Documentation
# Ouvrir http://localhost:8000/docs
```

---

## ðŸ“‹ Configuration Minimale .env

```env
JWT_SECRET_KEY=<gÃ©nÃ©rer>
ENVIRONMENT=development
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:1b
API_PORT=8000
```

**C'est suffisant pour tester !**

---

## ðŸŽ¯ Avantages

- âœ… **100% gratuit** - Ollama est gratuit
- âœ… **Local** - Pas besoin d'internet (aprÃ¨s installation)
- âœ… **Rapide** - Pas de latence rÃ©seau
- âœ… **PrivÃ©** - DonnÃ©es restent locales
- âœ… **Sans limites** - Pas de quotas

---

**Voir TEST_LOCAL_OLLAMA.md pour le guide complet**






