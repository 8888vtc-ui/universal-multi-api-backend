#!/bin/bash
# Script d'installation automatique d'Ollama + Llama
# Usage: sudo bash install_ollama.sh

set -e

# Couleurs
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}ü¶ô Installation d'Ollama + Llama${NC}"
echo "===================================="

# V√©rifier si on est root
if [ "$EUID" -ne 0 ]; then 
    echo -e "${RED}‚ùå Veuillez ex√©cuter en tant que root (sudo bash install_ollama.sh)${NC}"
    exit 1
fi

# V√©rifier la RAM disponible
RAM_GB=$(free -g | awk '/^Mem:/{print $2}')
echo -e "${YELLOW}üìä RAM disponible: ${RAM_GB}GB${NC}"

if [ "$RAM_GB" -lt 8 ]; then
    echo -e "${RED}‚ö†Ô∏è  Attention: Moins de 8GB RAM d√©tect√©. Llama 3.1 8B n√©cessite au moins 8GB.${NC}"
    read -p "Continuer quand m√™me? (y/n): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
fi

# Mettre √† jour le syst√®me
echo -e "${GREEN}[1/7] Mise √† jour du syst√®me...${NC}"
apt update && apt upgrade -y

# Installer les d√©pendances
echo -e "${GREEN}[2/7] Installation des d√©pendances...${NC}"
apt install -y curl git build-essential

# Installer Ollama
echo -e "${GREEN}[3/7] Installation d'Ollama...${NC}"
curl -fsSL https://ollama.com/install.sh | sh

# V√©rifier l'installation
if command -v ollama &> /dev/null; then
    echo -e "${GREEN}‚úÖ Ollama install√© avec succ√®s${NC}"
    ollama --version
else
    echo -e "${RED}‚ùå Erreur lors de l'installation d'Ollama${NC}"
    exit 1
fi

# Cr√©er l'utilisateur ollama
echo -e "${GREEN}[4/7] Cr√©ation de l'utilisateur ollama...${NC}"
if ! id "ollama" &>/dev/null; then
    useradd -r -s /bin/false -m -d /usr/share/ollama ollama
    echo -e "${GREEN}‚úÖ Utilisateur ollama cr√©√©${NC}"
else
    echo -e "${YELLOW}Utilisateur ollama existe d√©j√†${NC}"
fi

# Configurer Ollama comme service systemd
echo -e "${GREEN}[5/7] Configuration du service systemd...${NC}"
cat > /etc/systemd/system/ollama.service << 'EOF'
[Unit]
Description=Ollama Service
After=network.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="OLLAMA_HOST=0.0.0.0:11434"

[Install]
WantedBy=default.target
EOF

# Donner les permissions
chown -R ollama:ollama /usr/share/ollama 2>/dev/null || true

# Activer et d√©marrer le service
systemctl daemon-reload
systemctl enable ollama
systemctl start ollama

# Attendre que Ollama d√©marre
echo -e "${YELLOW}Attente du d√©marrage d'Ollama...${NC}"
sleep 5

# V√©rifier que le service fonctionne
if systemctl is-active --quiet ollama; then
    echo -e "${GREEN}‚úÖ Service Ollama d√©marr√©${NC}"
else
    echo -e "${RED}‚ùå Erreur: Le service Ollama ne d√©marre pas${NC}"
    systemctl status ollama
    exit 1
fi

# Configurer le firewall
echo -e "${GREEN}[6/7] Configuration du firewall...${NC}"
if command -v ufw &> /dev/null; then
    ufw allow 11434/tcp comment "Ollama"
    echo -e "${GREEN}‚úÖ Port 11434 ouvert dans le firewall${NC}"
else
    echo -e "${YELLOW}UFW non install√©, installation...${NC}"
    apt install -y ufw
    ufw allow 11434/tcp comment "Ollama"
    ufw allow 22/tcp comment "SSH"
    echo -e "${YELLOW}‚ö†Ô∏è  N'oubliez pas d'activer le firewall: sudo ufw enable${NC}"
fi

# T√©l√©charger Llama
echo -e "${GREEN}[7/7] T√©l√©chargement de Llama 3.1 8B...${NC}"
echo -e "${YELLOW}üì• Cela peut prendre plusieurs minutes (mod√®le ~4.7GB)...${NC}"

# T√©l√©charger en arri√®re-plan et afficher la progression
ollama pull llama3.1:8b &
OLLAMA_PID=$!

# Afficher un spinner pendant le t√©l√©chargement
spinner() {
    local pid=$1
    local spin='-\|/'
    local i=0
    while kill -0 $pid 2>/dev/null; do
        i=$(( (i+1) %4 ))
        printf "\r${YELLOW}T√©l√©chargement en cours... ${spin:$i:1}${NC}"
        sleep 0.1
    done
    printf "\r"
}

spinner $OLLAMA_PID
wait $OLLAMA_PID

# V√©rifier que le mod√®le est install√©
if ollama list | grep -q "llama3.1:8b"; then
    echo -e "${GREEN}‚úÖ Llama 3.1 8B install√© avec succ√®s${NC}"
else
    echo -e "${RED}‚ùå Erreur lors du t√©l√©chargement de Llama${NC}"
    exit 1
fi

# Afficher les mod√®les install√©s
echo ""
echo -e "${GREEN}üì¶ Mod√®les install√©s:${NC}"
ollama list

# Test rapide
echo ""
echo -e "${GREEN}üß™ Test rapide d'Ollama...${NC}"
echo -e "${YELLOW}Test: 'Bonjour, comment √ßa va?'${NC}"
ollama run llama3.1:8b "Bonjour, comment √ßa va?" --verbose 2>&1 | head -5

# Afficher les informations de connexion
echo ""
echo -e "${GREEN}‚úÖ Installation termin√©e avec succ√®s !${NC}"
echo ""
echo -e "${BLUE}üìã Informations de connexion:${NC}"
echo -e "  URL locale: http://localhost:11434"
echo -e "  URL externe: http://$(hostname -I | awk '{print $1}'):11434"
echo ""
echo -e "${BLUE}üîß Commandes utiles:${NC}"
echo -e "  Status: sudo systemctl status ollama"
echo -e "  Logs: sudo journalctl -u ollama -f"
echo -e "  Red√©marrer: sudo systemctl restart ollama"
echo -e "  Tester: ollama run llama3.1:8b 'Votre question'"
echo ""
echo -e "${BLUE}üîó Pour votre backend, ajoutez dans .env:${NC}"
echo -e "  OLLAMA_BASE_URL=http://$(hostname -I | awk '{print $1}'):11434"
echo ""
echo -e "${YELLOW}‚ö†Ô∏è  S√©curit√©: N'oubliez pas de s√©curiser l'acc√®s √† Ollama en production !${NC}"


<<<<<<< Updated upstream
=======





>>>>>>> Stashed changes
