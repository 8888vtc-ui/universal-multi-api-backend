# üß™ Test Local avec Ollama - Guide Complet

**Date** : D√©cembre 2024  
**Version** : 2.3.0  
**Objectif** : Tester localement avant le d√©ploiement, avec Ollama gratuit

---

## üéØ Pourquoi Tester Localement avec Ollama ?

- ‚úÖ **100% gratuit** - Pas besoin de cl√©s API
- ‚úÖ **Local** - Fonctionne sans internet (apr√®s installation)
- ‚úÖ **Rapide** - Pas de latence r√©seau
- ‚úÖ **Parfait pour tester** - Valide tout avant production
- ‚úÖ **Ollama inclus** - IA locale gratuite

---

## üìã Pr√©requis

- Python 3.9+
- Docker (optionnel, pour Ollama)
- 8GB+ RAM recommand√© (pour Ollama)
- Windows/Mac/Linux

---

## üöÄ Installation Rapide

### √âtape 1 : Installer Ollama (5 min)

#### Windows
```powershell
# T√©l√©charger depuis https://ollama.com/download
# Installer l'ex√©cutable
# Ollama d√©marrera automatiquement
```

#### Mac
```bash
# Avec Homebrew
brew install ollama

# Ou t√©l√©charger depuis https://ollama.com/download
```

#### Linux
```bash
# Installation automatique
curl -fsSL https://ollama.com/install.sh | sh

# Ou avec le script du projet
cd backend
bash install_ollama.sh
```

### √âtape 2 : T√©l√©charger un Mod√®le (10-20 min)

```bash
# Mod√®le l√©ger (recommand√© pour test)
ollama pull llama3.2:1b

# Ou mod√®le plus performant
ollama pull llama3.2:3b

# Ou mod√®le complet (plus lent mais meilleur)
ollama pull llama3.2
```

**Note** : Le premier t√©l√©chargement peut prendre du temps selon votre connexion.

### √âtape 3 : V√©rifier Ollama

```bash
# Tester Ollama
ollama run llama3.2:1b "Hello, how are you?"

# V√©rifier que le serveur tourne
curl http://localhost:11434/api/tags
```

---

## ‚öôÔ∏è Configuration du Backend

### √âtape 1 : Configurer .env

```bash
cd backend

# Copier .env.example
cp .env.example .env

# √âditer .env
nano .env  # ou votre √©diteur pr√©f√©r√©
```

### √âtape 2 : Configuration Minimale pour Test Local

Dans `.env`, configurez au minimum :

```env
# Obligatoire
JWT_SECRET_KEY=<g√©n√©rer avec: python -c "import secrets; print(secrets.token_urlsafe(32))">
ENVIRONMENT=development

# Ollama (gratuit, local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:1b

# Port
API_PORT=8000
API_HOST=0.0.0.0

# Optionnel (pour tester d'autres fonctionnalit√©s)
# GROQ_API_KEY=  # Si vous voulez tester Groq aussi
# MISTRAL_API_KEY=  # Si vous voulez tester Mistral aussi
```

**Important** : Ollama fonctionne sans cl√© API, c'est gratuit et local !

---

## üöÄ D√©marrer le Serveur

### Option 1 : Script de D√©marrage (Recommand√©)

```bash
cd backend
python scripts/start_server.py
```

### Option 2 : Directement

```bash
cd backend
python main.py
```

### Option 3 : Avec Uvicorn

```bash
cd backend
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

---

## ‚úÖ V√©rification

### 1. V√©rifier que le Serveur D√©marre

```bash
# Le serveur devrait afficher :
# üöÄ Universal Multi-API Backend v2.3.0
# üì° Server: http://0.0.0.0:8000
# üìö Docs: http://0.0.0.0:8000/docs
```

### 2. Tester les Endpoints

```bash
# Health check
curl http://localhost:8000/api/health

# API Info
curl http://localhost:8000/api/info

# Chat avec Ollama (gratuit, local)
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message":"Hello, how are you?","language":"en"}'
```

### 3. Tester depuis le Navigateur

Ouvrir :
- **Documentation** : http://localhost:8000/docs
- **Health** : http://localhost:8000/api/health
- **Metrics** : http://localhost:8000/api/metrics

---

## üß™ Tests Complets

### Test 1 : Chat IA avec Ollama

```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Explique-moi ce qu est l intelligence artificielle",
    "language": "fr"
  }'
```

### Test 2 : Health Checks

```bash
# Health simple
curl http://localhost:8000/api/health

# Deep health
curl http://localhost:8000/api/health/deep

# Kubernetes probes
curl http://localhost:8000/api/health/ready
curl http://localhost:8000/api/health/live
```

### Test 3 : Metrics

```bash
# Metrics JSON
curl http://localhost:8000/api/metrics

# Prometheus
curl http://localhost:8000/api/metrics/prometheus
```

### Test 4 : Recherche Universelle

```bash
curl -X POST http://localhost:8000/api/search/universal \
  -H "Content-Type: application/json" \
  -d '{
    "query": "test",
    "categories": ["news"],
    "max_results_per_category": 3
  }'
```

---

## üîß Configuration Avanc√©e Ollama

### Mod√®les Disponibles

```bash
# Lister les mod√®les disponibles
ollama list

# Mod√®les recommand√©s pour test :
# - llama3.2:1b (tr√®s rapide, l√©ger)
# - llama3.2:3b (bon √©quilibre)
# - llama3.2 (complet, plus lent)
# - mistral (alternatif)
# - phi3 (Microsoft, l√©ger)
```

### Changer le Mod√®le

Dans `.env` :
```env
OLLAMA_MODEL=llama3.2:3b  # Changer le mod√®le
```

Red√©marrer le serveur.

---

## üìä Performance Locale

### Avantages du Test Local

- ‚úÖ **Rapide** - Pas de latence r√©seau
- ‚úÖ **Gratuit** - Ollama est gratuit
- ‚úÖ **Priv√©** - Donn√©es restent locales
- ‚úÖ **Sans limites** - Pas de quotas API
- ‚úÖ **Test complet** - Toutes les fonctionnalit√©s

### Limitations

- ‚ö†Ô∏è **Ressources** - Ollama utilise de la RAM/CPU
- ‚ö†Ô∏è **Mod√®les** - Plus petits que les mod√®les cloud
- ‚ö†Ô∏è **Performance** - D√©pend de votre machine

---

## üêõ D√©pannage

### Ollama Ne D√©marre Pas

```bash
# V√©rifier que Ollama tourne
ollama serve

# Ou red√©marrer
ollama serve
```

### Le Serveur Ne Peut Pas Contacter Ollama

```bash
# V√©rifier l'URL dans .env
cat backend/.env | grep OLLAMA

# Tester Ollama directement
curl http://localhost:11434/api/tags

# V√©rifier le port
netstat -an | grep 11434
```

### Erreur "Model not found"

```bash
# T√©l√©charger le mod√®le
ollama pull llama3.2:1b

# V√©rifier les mod√®les disponibles
ollama list
```

### Probl√®mes de M√©moire

Si vous avez peu de RAM :
```bash
# Utiliser un mod√®le plus petit
ollama pull llama3.2:1b  # Au lieu de llama3.2

# Dans .env
OLLAMA_MODEL=llama3.2:1b
```

---

## üìã Checklist de Test Local

### Avant de Commencer
- [ ] Python 3.9+ install√©
- [ ] Ollama install√©
- [ ] Mod√®le Ollama t√©l√©charg√©
- [ ] .env configur√©

### D√©marrage
- [ ] Ollama d√©marr√©
- [ ] Serveur backend d√©marr√©
- [ ] Pas d'erreurs dans les logs

### Tests
- [ ] Health checks OK
- [ ] Chat avec Ollama fonctionne
- [ ] Documentation accessible
- [ ] Metrics accessibles
- [ ] Tous les endpoints test√©s

---

## üéØ Workflow Recommand√©

### 1. Test Local (Maintenant)
```bash
# Tester localement avec Ollama
# Valider toutes les fonctionnalit√©s
# Corriger les bugs
```

### 2. Test sur Serveur Gratuit (Oracle Cloud)
```bash
# D√©ployer sur Oracle Cloud Free Tier
# Tester avec du trafic r√©el
# Valider la performance
```

### 3. Production (Hetzner)
```bash
# D√©ployer en production
# Avec toutes les cl√©s API
# Monitoring configur√©
```

---

## üí° Astuces

### Optimiser Ollama

```bash
# Utiliser un mod√®le plus petit pour test
ollama pull llama3.2:1b

# Limiter la RAM utilis√©e
export OLLAMA_NUM_GPU=0  # D√©sactiver GPU si probl√®me
```

### Tester avec Plusieurs Mod√®les

```bash
# T√©l√©charger plusieurs mod√®les
ollama pull llama3.2:1b
ollama pull mistral
ollama pull phi3

# Changer dans .env pour tester
OLLAMA_MODEL=mistral
```

---

## üìö Documentation

- **Ollama** : https://ollama.com
- **Mod√®les** : https://ollama.com/library
- **API** : http://localhost:8000/docs

---

## ‚úÖ R√©sultat Attendu

Apr√®s ce test local, vous devriez avoir :
- ‚úÖ Serveur fonctionnel localement
- ‚úÖ Chat IA avec Ollama fonctionnel
- ‚úÖ Tous les endpoints test√©s
- ‚úÖ Confiance pour d√©ployer

---

**Bon test local ! üöÄ**

*Derni√®re mise √† jour : D√©cembre 2024 - v2.3.0*






