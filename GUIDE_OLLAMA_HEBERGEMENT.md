# ü¶ô GUIDE H√âBERGEMENT OLLAMA + LLAMA
## Pourquoi Railway ne fonctionne pas et les meilleures alternatives

---

## ‚ùå **POURQUOI RAILWAY NE FONCTIONNE PAS**

### **Probl√®mes avec Railway pour Ollama :**

1. **Pas d'acc√®s root** : Railway est un PaaS (Platform as a Service)
   - ‚ùå Impossible d'installer Ollama (n√©cessite acc√®s syst√®me)
   - ‚ùå Pas de contr√¥le sur l'environnement syst√®me

2. **Limites de ressources** :
   - ‚ùå RAM limit√©e (g√©n√©ralement < 2GB)
   - ‚ùå Llama 3.1 n√©cessite **4-8GB RAM minimum**
   - ‚ùå Pas de GPU disponible

3. **Pas de persistance** :
   - ‚ùå Les mod√®les Llama sont gros (4-8GB)
   - ‚ùå Pas de stockage persistant pour les mod√®les

4. **Timeout** :
   - ‚ùå Ollama peut prendre du temps √† r√©pondre
   - ‚ùå Railway a des timeouts stricts

---

## ‚úÖ **SOLUTION : VPS D√âDI√â**

**Vous avez BESOIN d'un VPS o√π vous contr√¥lez tout !**

---

## üéØ **MEILLEURES OPTIONS POUR OLLAMA + LLAMA**

### **üìä EXIGENCES MINIMALES**

Pour faire tourner **Llama 3.1** avec Ollama :

- **RAM** : **8GB minimum** (16GB recommand√©)
- **CPU** : **4 vCPU minimum** (8 vCPU recommand√©)
- **Stockage** : **100GB minimum** (pour mod√®les + syst√®me)
- **OS** : Ubuntu 22.04 LTS (recommand√©)

---

### **üèÜ TOP 3 RECOMMANDATIONS**

#### **1. Hetzner Cloud CPX21** ‚≠ê MEILLEUR CHOIX
- **Prix** : **9.50‚Ç¨/mois** (ou 6.50‚Ç¨/mois avec r√©servation annuelle)
- **Sp√©cifications** :
  - CPU : 3 vCPU AMD EPYC
  - RAM : **8GB** ‚úÖ
  - Stockage : 160GB NVMe SSD ‚úÖ
  - R√©seau : 20TB/mois
- **Avantages** :
  - ‚úÖ Parfait pour Llama 3.1 (8GB RAM)
  - ‚úÖ Excellent rapport qualit√©/prix
  - ‚úÖ Performance CPU excellente
  - ‚úÖ Stockage rapide (NVMe)
- **Mod√®les support√©s** :
  - Llama 3.1 8B (parfait)
  - Llama 3.1 70B (trop gros, n√©cessite 16GB+)
- **URL** : https://www.hetzner.com/cloud

---

#### **2. Hetzner Cloud CPX31** (Si besoin de plus de puissance)
- **Prix** : **15.21‚Ç¨/mois**
- **Sp√©cifications** :
  - CPU : 4 vCPU AMD EPYC Genoa
  - RAM : **8GB**
  - Stockage : 160GB NVMe SSD
- **Quand choisir** : Si vous voulez plus de CPU pour traitement parall√®le

---

#### **3. Hetzner Cloud CCX23** (Si vous voulez Llama 70B)
- **Prix** : **23.44‚Ç¨/mois**
- **Sp√©cifications** :
  - CPU : 4 vCPU AMD EPYC
  - RAM : **16GB** ‚úÖ‚úÖ
  - Stockage : 240GB NVMe SSD
- **Quand choisir** : Pour Llama 3.1 70B ou plusieurs mod√®les en parall√®le

---

### **üí∞ ALTERNATIVES √âCONOMIQUES**

#### **Contabo VPS L** (Si budget serr√©)
- **Prix** : **8.99‚Ç¨/mois**
- **Sp√©cifications** :
  - CPU : 6 vCPU
  - RAM : **8GB**
  - Stockage : 200GB SSD
- **Avantages** :
  - ‚úÖ Moins cher
  - ‚úÖ Plus de stockage
- **Inconv√©nients** :
  - ‚ö†Ô∏è Performance CPU moins bonne que Hetzner
  - ‚ö†Ô∏è Support moins r√©actif
- **URL** : https://www.contabo.com

---

## üìã **GUIDE D'INSTALLATION COMPLET**

### **√âtape 1 : Cr√©er le serveur Hetzner**

1. Aller sur https://www.hetzner.com/cloud
2. Cr√©er un compte
3. Cr√©er un nouveau projet
4. Ajouter un serveur :
   - **Type** : **CPX21** (9.50‚Ç¨/mois) ou **CPX31** (15.21‚Ç¨/mois)
   - **OS** : Ubuntu 22.04 LTS
   - **Localisation** : Falkenstein (Allemagne) ou Nuremberg
   - **SSH Key** : Ajouter votre cl√© SSH

### **√âtape 2 : Se connecter au serveur**

```bash
ssh root@VOTRE_IP
```

### **√âtape 3 : Installer Ollama**

```bash
# Mettre √† jour le syst√®me
apt update && apt upgrade -y

# Installer les d√©pendances
apt install -y curl git build-essential

# Installer Ollama
curl -fsSL https://ollama.com/install.sh | sh

# V√©rifier l'installation
ollama --version
```

### **√âtape 4 : T√©l√©charger Llama 3.1**

```bash
# T√©l√©charger Llama 3.1 8B (recommand√© pour 8GB RAM)
ollama pull llama3.1:8b

# OU Llama 3.1 70B (n√©cessite 16GB+ RAM)
# ollama pull llama3.1:70b

# V√©rifier les mod√®les install√©s
ollama list
```

### **√âtape 5 : Tester Ollama**

```bash
# Test simple
ollama run llama3.1:8b "Bonjour, comment √ßa va?"

# D√©marrer Ollama en service (pour qu'il tourne toujours)
# Voir √©tape suivante
```

### **√âtape 6 : Configurer Ollama comme service systemd**

```bash
# Cr√©er le fichier service
sudo nano /etc/systemd/system/ollama.service
```

**Contenu** :
```ini
[Unit]
Description=Ollama Service
After=network.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3

[Install]
WantedBy=default.target
```

**Activer le service** :
```bash
# Cr√©er l'utilisateur ollama
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

# Donner les permissions
sudo chown -R ollama:ollama /usr/share/ollama

# Activer et d√©marrer
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama
sudo systemctl status ollama
```

### **√âtape 7 : Configurer le firewall**

```bash
# Autoriser le port Ollama (11434)
sudo ufw allow 11434/tcp

# Autoriser SSH
sudo ufw allow 22/tcp

# Activer le firewall
sudo ufw enable
```

### **√âtape 8 : Tester depuis l'ext√©rieur**

```bash
# Depuis votre machine locale
curl http://VOTRE_IP:11434/api/tags

# Devrait retourner la liste des mod√®les
```

---

## üîß **CONFIGURER VOTRE BACKEND POUR OLLAMA**

### **Modifier votre `.env`**

```bash
# Dans votre backend/.env
OLLAMA_BASE_URL=http://VOTRE_IP:11434
# OU si vous utilisez un domaine
# OLLAMA_BASE_URL=http://ollama.votre-domaine.com:11434
```

### **V√©rifier que √ßa fonctionne**

```bash
# Dans votre backend
python -c "
import httpx
response = httpx.get('http://VOTRE_IP:11434/api/tags')
print(response.json())
"
```

---

## üöÄ **OPTIMISATION POUR PERFORMANCE**

### **1. Utiliser plusieurs mod√®les en parall√®le**

```bash
# T√©l√©charger plusieurs mod√®les
ollama pull llama3.1:8b
ollama pull mistral:7b
ollama pull codellama:7b

# Votre backend peut utiliser celui qui r√©pond le mieux
```

### **2. Optimiser la m√©moire**

```bash
# Limiter le nombre de mod√®les charg√©s en m√©moire
# Ollama charge automatiquement les mod√®les les plus utilis√©s

# Voir l'utilisation m√©moire
ollama ps
```

### **3. Utiliser un reverse proxy (Nginx)**

Pour s√©curiser l'acc√®s √† Ollama :

```bash
# Installer Nginx
sudo apt install -y nginx

# Cr√©er la config
sudo nano /etc/nginx/sites-available/ollama
```

**Contenu** :
```nginx
server {
    listen 80;
    server_name votre-domaine.com;

    location / {
        proxy_pass http://localhost:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    }
}
```

---

## üìä **COMPARAISON DES OPTIONS**

| Provider | Prix | RAM | CPU | Stockage | Note pour Ollama |
|----------|------|-----|-----|----------|------------------|
| **Hetzner CPX21** | 9.50‚Ç¨ | 8GB | 3 vCPU | 160GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Parfait |
| **Hetzner CPX31** | 15.21‚Ç¨ | 8GB | 4 vCPU | 160GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Plus de CPU |
| **Hetzner CCX23** | 23.44‚Ç¨ | 16GB | 4 vCPU | 240GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Pour Llama 70B |
| **Contabo VPS L** | 8.99‚Ç¨ | 8GB | 6 vCPU | 200GB | ‚≠ê‚≠ê‚≠ê‚≠ê √âconomique |

---

## üí° **MA RECOMMANDATION FINALE**

### **Pour Ollama + Llama 3.1 8B**

**Hetzner Cloud CPX21 √† 9.50‚Ç¨/mois** ‚≠ê

**Pourquoi** :
- ‚úÖ 8GB RAM = parfait pour Llama 3.1 8B
- ‚úÖ 160GB stockage = suffisant pour plusieurs mod√®les
- ‚úÖ Performance excellente
- ‚úÖ Prix raisonnable
- ‚úÖ Fiable et stable

### **Si vous voulez Llama 70B**

**Hetzner Cloud CCX23 √† 23.44‚Ç¨/mois**

**Pourquoi** :
- ‚úÖ 16GB RAM n√©cessaire pour Llama 70B
- ‚úÖ Plus de stockage pour gros mod√®les

---

## üéØ **PLAN D'ACTION**

1. ‚úÖ Cr√©er un compte Hetzner Cloud
2. ‚úÖ Cr√©er un serveur CPX21 (9.50‚Ç¨/mois)
3. ‚úÖ Installer Ollama (script ci-dessus)
4. ‚úÖ T√©l√©charger Llama 3.1 8B
5. ‚úÖ Configurer votre backend pour utiliser Ollama
6. ‚úÖ Tester depuis vos projets (bot backgammon, etc.)

---

## üîí **S√âCURIT√â**

### **Important : S√©curiser Ollama**

Par d√©faut, Ollama est accessible √† tous sur le port 11434. Pour la production :

1. **Utiliser un reverse proxy avec authentification**
2. **Restreindre l'acc√®s par IP** (firewall)
3. **Utiliser HTTPS** (Let's Encrypt)
4. **Ne pas exposer Ollama directement** (utiliser votre backend comme proxy)

---

## üìù **SCRIPT D'INSTALLATION AUTOMATIQUE**

Je peux cr√©er un script qui fait tout automatiquement :
- Installation Ollama
- T√©l√©chargement Llama
- Configuration service systemd
- Configuration Nginx
- Configuration firewall

**Voulez-vous que je cr√©e ce script ?**

---

## ‚úÖ **R√âSUM√â**

- ‚ùå **Railway** : Ne fonctionne PAS pour Ollama
- ‚úÖ **Hetzner CPX21** : Meilleur choix (9.50‚Ç¨/mois)
- ‚úÖ **8GB RAM minimum** pour Llama 3.1 8B
- ‚úÖ **VPS n√©cessaire** pour installer Ollama
- ‚úÖ **Ollama local** = illimit√©, gratuit, priv√© !

**Avec Ollama + Llama local, vous n'avez plus besoin de quotas d'API ! üéâ**


